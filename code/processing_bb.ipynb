{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from scipy.stats import skew\n",
    "import shutil\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler,MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "def download_uci_datasets(dataset_ids: list, directory_path: str):\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "    for dataset_id in dataset_ids:\n",
    "        dataset = fetch_ucirepo(id=dataset_id)\n",
    "\n",
    "        # Pobierz WSZYSTKIE dane, nie tylko features i targets\n",
    "        df = dataset.data.original.copy()\n",
    "\n",
    "        dataset_name = dataset.metadata.name.replace(\" \", \"_\")\n",
    "        filename_base = f\"{dataset_name}_{dataset_id}\"\n",
    "        csv_path = os.path.join(directory_path, f\"{filename_base}.csv\")\n",
    "        meta_path = os.path.join(directory_path, f\"{filename_base}.meta.json\")\n",
    "\n",
    "        # Zapis danych .csv\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        # WyciƒÖgamy info z metadanych\n",
    "        variables = dataset.variables\n",
    "        features = variables[variables[\"role\"] == \"Feature\"][\"name\"].tolist()\n",
    "        targets = variables[variables[\"role\"] == \"Target\"][\"name\"].tolist()\n",
    "        others = variables[variables[\"role\"] == \"Other\"][\"name\"].tolist()\n",
    "\n",
    "        metadata = {\n",
    "            \"dataset_name\": dataset.metadata.name,\n",
    "            \"id\": int(dataset_id),\n",
    "            \"num_rows\": int(df.shape[0]),\n",
    "            \"num_columns\": int(df.shape[1]),\n",
    "            \"columns\": df.columns.tolist(),\n",
    "            \"features\": features,\n",
    "            \"targets\": targets,\n",
    "            \"others\": others,\n",
    "            \"source\": \"UCI\"\n",
    "        }\n",
    "\n",
    "        # Bezpieczna konwersja typ√≥w do JSON\n",
    "        def convert(o):\n",
    "            if isinstance(o, (np.generic, np.ndarray)):\n",
    "                return o.item() if hasattr(o, 'item') else o.tolist()\n",
    "            return o\n",
    "\n",
    "        metadata = {k: convert(v) for k, v in metadata.items()}\n",
    "\n",
    "        with open(meta_path, \"w\") as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "\n",
    "        print(f\"‚úî Saved CSV: {csv_path}\")\n",
    "        print(f\"‚úî Saved Metadata: {meta_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def download_uci_datasets(dataset_ids: list, directory_path: str):\n",
    "#     os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "#     for dataset_id in dataset_ids:\n",
    "#         dataset = fetch_ucirepo(id=dataset_id)\n",
    "\n",
    "#         X = dataset.data.features\n",
    "#         y = dataset.data.targets\n",
    "\n",
    "#         # Je≈õli target to Series, zamieniamy na DataFrame\n",
    "#         if isinstance(y, pd.Series):\n",
    "#             y = y.to_frame(name=y.name or 'target')\n",
    "\n",
    "#         # ‚¨á ZACHOWUJEMY WSZYSTKIE TARGETY\n",
    "#         df = pd.concat([X, y], axis=1)\n",
    "\n",
    "#         dataset_name = dataset.metadata.name.replace(\" \", \"_\")\n",
    "#         filename_base = f\"{dataset_name}_{dataset_id}\"\n",
    "#         csv_path = os.path.join(directory_path, f\"{filename_base}.csv\")\n",
    "#         meta_path = os.path.join(directory_path, f\"{filename_base}.meta.json\")\n",
    "\n",
    "#         # Zapis CSV z pe≈Çnymi danymi\n",
    "#         df.to_csv(csv_path, index=False)\n",
    "\n",
    "#         # Proste metadane, tylko ≈ºeby nie by≈Ço b≈Çƒôd√≥w z typami\n",
    "#         metadata = {\n",
    "#             \"dataset_name\": dataset.metadata.name,\n",
    "#             \"id\": int(dataset_id),\n",
    "#             \"num_rows\": int(df.shape[0]),\n",
    "#             \"num_columns\": int(df.shape[1]),\n",
    "#             \"columns\": df.columns.tolist(),\n",
    "#             \"features\": X.columns.tolist(),\n",
    "#             \"targets\": y.columns.tolist(),\n",
    "#             \"source\": \"UCI\"\n",
    "#         }\n",
    "\n",
    "#         # Konwersja do typ√≥w JSON-friendly\n",
    "#         def convert(o):\n",
    "#             if isinstance(o, (np.generic, np.ndarray)):\n",
    "#                 return o.item() if hasattr(o, 'item') else o.tolist()\n",
    "#             return o\n",
    "\n",
    "#         metadata = {k: convert(v) for k, v in metadata.items()}\n",
    "\n",
    "#         with open(meta_path, \"w\") as f:\n",
    "#             json.dump(metadata, f, indent=2)\n",
    "\n",
    "#         print(f\"‚úî Saved CSV: {csv_path}\")\n",
    "#         print(f\"‚úî Saved Metadata: {meta_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_uci_datasets(dataset_ids: list, directory_path: str):\n",
    "#     \"\"\"\n",
    "#     Pobiera zestawy danych UCI, zapisuje dane w formacie CSV oraz tworzy metadane w formacie JSON.\n",
    "\n",
    "#     Funkcja wykonuje nastƒôpujƒÖce kroki dla ka≈ºdego datasetu:\n",
    "#       1. Tworzy katalog docelowy, je≈õli nie istnieje.\n",
    "#       2. Pobiera dane (cechy oraz zmiennƒÖ target) za pomocƒÖ funkcji fetch_ucirepo.\n",
    "#       3. Wybiera pierwszƒÖ numerycznƒÖ kolumnƒô target, opierajƒÖc siƒô na metadanych datasetu.\n",
    "#       4. ≈ÅƒÖczy cechy i wybranƒÖ zmiennƒÖ target w jeden DataFrame.\n",
    "#       5. Zapisuje DataFrame jako plik CSV.\n",
    "#       6. Generuje metadane, zawierajƒÖce informacje o datasetcie takie jak liczba cech, statystyki targetu, korelacje cech z targetem oraz procent brakujƒÖcych warto≈õci.\n",
    "#       7. Zapisuje metadane w formacie JSON.\n",
    "\n",
    "#     Dziƒôki temu funkcja umo≈ºliwia wygodne pobieranie oraz przygotowywanie dataset√≥w z repozytorium UCI, gotowych do dalszej analizy i eksperyment√≥w.\n",
    "#     \"\"\"\n",
    "#     os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "#     for dataset_id in dataset_ids:\n",
    "#         dataset = fetch_ucirepo(id=dataset_id)\n",
    "\n",
    "#         X = dataset.data.features\n",
    "#         y = dataset.data.targets\n",
    "\n",
    "#         if isinstance(y, pd.Series):\n",
    "#             y = y.to_frame(name=y.name or 'target')\n",
    "\n",
    "#         # Extract numeric target from metadata\n",
    "#         variables_metadata = dataset.variables\n",
    "\n",
    "#         numeric_target_cols = variables_metadata[\n",
    "#             (variables_metadata['role'] == 'Target') &\n",
    "#             (variables_metadata['type'].isin(['Integer', 'Numeric', 'Real']))\n",
    "#         ]['name'].tolist()\n",
    "\n",
    "#         # Choose the numeric target column explicitly defined in metadata\n",
    "#         if numeric_target_cols:\n",
    "#             target_col = numeric_target_cols[0]\n",
    "#         elif isinstance(y, pd.DataFrame) and y.shape[1] > 1:\n",
    "#             numeric_targets = y.select_dtypes(include=[np.number]).columns\n",
    "#             target_col = numeric_targets[0] if not numeric_targets.empty else y.columns[0]\n",
    "#         else:\n",
    "#             target_col = y.columns[0] if isinstance(y, pd.DataFrame) else 'target'\n",
    "\n",
    "#         y = y[[target_col]]\n",
    "\n",
    "#         # Combine X and y\n",
    "#         df = pd.concat([X, y], axis=1)\n",
    "\n",
    "#         dataset_name = dataset.metadata.name.replace(\" \", \"_\")\n",
    "#         filename_base = f\"{dataset_name}_{dataset_id}\"\n",
    "#         csv_path = os.path.join(directory_path, f\"{filename_base}.csv\")\n",
    "#         meta_path = os.path.join(directory_path, f\"{filename_base}.meta.json\")\n",
    "\n",
    "#         df.to_csv(csv_path, index=False)\n",
    "\n",
    "#         feature_cols = X.columns.tolist()\n",
    "#         numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "#         categorical_features = [col for col in feature_cols if col not in numeric_features]\n",
    "\n",
    "#         n_rows, n_cols = df.shape\n",
    "#         null_pct = df.isnull().sum().sum() / df.size * 100\n",
    "\n",
    "#         target_series = y[target_col].dropna()\n",
    "#         if pd.api.types.is_numeric_dtype(target_series):\n",
    "#             target_mean = target_series.mean()\n",
    "#             target_median = target_series.median()\n",
    "#             target_min = target_series.min()\n",
    "#             target_max = target_series.max()\n",
    "#             target_std = target_series.std()\n",
    "#             target_skew = skew(target_series)\n",
    "#             target_cv = target_std / (target_mean + 1e-8)\n",
    "#             outliers = ((target_series - target_mean).abs() > 3 * target_std).sum()\n",
    "#             outlier_ratio = outliers / len(target_series) * 100\n",
    "#             q1 = np.percentile(target_series, 25)\n",
    "#             q3 = np.percentile(target_series, 75)\n",
    "#             iqr = round(q3 - q1, 3)\n",
    "#             unique_vals = target_series.nunique()\n",
    "#         else:\n",
    "#             target_mean = target_median = target_min = target_max = None\n",
    "#             target_std = target_skew = target_cv = None\n",
    "#             outlier_ratio = iqr = unique_vals = None\n",
    "\n",
    "#         corr_with_target = {}\n",
    "#         if target_std is not None:\n",
    "#             for col in numeric_features:\n",
    "#                 try:\n",
    "#                     corr = df[[col, target_col]].dropna().corr().iloc[0, 1]\n",
    "#                     corr_with_target[col] = round(corr, 3)\n",
    "#                 except:\n",
    "#                     continue\n",
    "\n",
    "#         metadata = {\n",
    "#             \"dataset_name\": dataset.metadata.name,\n",
    "#             \"id\": dataset_id,\n",
    "#             \"target\": target_col,\n",
    "#             \"num_observations\": n_rows,\n",
    "#             \"num_features\": len(feature_cols),\n",
    "#             \"numeric_features\": numeric_features,\n",
    "#             \"categorical_features\": categorical_features,\n",
    "#             \"%_null_values\": round(null_pct, 2),\n",
    "#             \"target_mean\": round(target_mean, 3) if target_mean is not None else None,\n",
    "#             \"target_median\": round(target_median, 3) if target_median is not None else None,\n",
    "#             \"target_min\": target_min,\n",
    "#             \"target_max\": target_max,\n",
    "#             \"target_std_dev\": round(target_std, 3) if target_std is not None else None,\n",
    "#             \"target_skewness\": round(target_skew, 3) if target_skew is not None else None,\n",
    "#             \"target_coefficient_of_variation\": round(target_cv, 3) if target_cv is not None else None,\n",
    "#             \"target_outlier_%\": round(outlier_ratio, 2) if outlier_ratio is not None else None,\n",
    "#             \"target_IQR\": iqr,\n",
    "#             \"target_unique_values\": unique_vals,\n",
    "#             \"feature_target_correlations\": corr_with_target,\n",
    "#             \"source\": \"UCI\"\n",
    "#         }\n",
    "\n",
    "#         def convert(o):\n",
    "#             if isinstance(o, (np.generic, np.ndarray)):\n",
    "#                 return o.item() if hasattr(o, 'item') else o.tolist()\n",
    "#             return o\n",
    "\n",
    "#         metadata = {k: convert(v) for k, v in metadata.items()}\n",
    "\n",
    "#         with open(meta_path, \"w\") as f:\n",
    "#             json.dump(metadata, f, indent=2)\n",
    "\n",
    "#         print(f\"‚úî Zapisano {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_summary_table(input_datasets_directory: str, output_table_directory: str):\n",
    "    \"\"\"\n",
    "    Funkcja generuje tabelƒô zbiorczƒÖ podsumowujƒÖcƒÖ metadane dataset√≥w, kt√≥rych pliki metadanych (.meta.json)\n",
    "    znajdujƒÖ siƒô w katalogu input_datasets_directory.\n",
    "\n",
    "    Dzia≈Çanie funkcji:\n",
    "      1. Tworzy katalog wyj≈õciowy (output_table_directory), je≈õli nie istnieje.\n",
    "      2. Iteruje przez wszystkie pliki .meta.json w katalogu wej≈õciowym.\n",
    "      3. Dla ka≈ºdego pliku metadanych:\n",
    "         - Wczytuje metadane zapisane w formacie JSON.\n",
    "         - Wyodrƒôbnia istotne informacje, takie jak nazwa datasetu, ≈∫r√≥d≈Ço, liczba obserwacji, liczba cech,\n",
    "           liczba cech numerycznych i kategorycznych, procent brakujƒÖcych warto≈õci oraz statystyki zmiennej target.\n",
    "         - Dla cech numerycznych, kt√≥re korelujƒÖ ze zmiennƒÖ target, okre≈õla tƒô o najwiƒôkszej bezwzglƒôdnej warto≈õci\n",
    "           korelacji (tzw. Top Correlated Feature).\n",
    "      4. ≈ÅƒÖczy zebrane informacje w jeden DataFrame.\n",
    "      5. Sortuje tabelƒô wed≈Çug liczby obserwacji oraz liczby cech.\n",
    "      6. Zapisuje finalnƒÖ tabelƒô podsumowujƒÖcƒÖ w formacie CSV w katalogu output_table_directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_table_directory, exist_ok=True)\n",
    "\n",
    "    summary = []\n",
    "\n",
    "    for file in os.listdir(input_datasets_directory):\n",
    "        if file.endswith(\".meta.json\"):\n",
    "            meta_path = os.path.join(input_datasets_directory, file)\n",
    "\n",
    "            with open(meta_path) as f:\n",
    "                metadata = json.load(f)\n",
    "\n",
    "            corr_dict = metadata.get(\"feature_target_correlations\", {})\n",
    "            if corr_dict:\n",
    "                top_corr_feature = max(corr_dict.items(), key=lambda x: abs(x[1]))\n",
    "                top_corr_name = top_corr_feature[0]\n",
    "                top_corr_value = top_corr_feature[1]\n",
    "            else:\n",
    "                top_corr_name = None\n",
    "                top_corr_value = None\n",
    "\n",
    "            summary.append({\n",
    "                \"Dataset Name\": metadata.get(\"dataset_name\"),\n",
    "                \"Source\": metadata.get(\"source\", \"OpenML\"),\n",
    "                \"ID/Code\": metadata.get(\"code\") or metadata.get(\"id\"),\n",
    "                \"Num Observations\": metadata.get(\"num_observations\"),\n",
    "                \"Num Features\": metadata.get(\"num_features\"),\n",
    "                \"Categorical Features\": len(metadata.get(\"categorical_features\", [])),\n",
    "                \"Numeric Features\": len(metadata.get(\"numeric_features\", [])),\n",
    "                \"% Null Values\": metadata.get(\"%_null_values\"),\n",
    "                \"Target\": metadata.get(\"target\"),\n",
    "                \"Target Mean\": metadata.get(\"target_mean\"),\n",
    "                \"Target Median\": metadata.get(\"target_median\"),\n",
    "                \"Target Min\": metadata.get(\"target_min\"),\n",
    "                \"Target Max\": metadata.get(\"target_max\"),\n",
    "                \"Target Std Dev\": metadata.get(\"target_std_dev\"),\n",
    "                \"Target IQR\": metadata.get(\"target_IQR\"),\n",
    "                \"Target Skewness\": metadata.get(\"target_skewness\"),\n",
    "                \"Target Outlier %\": metadata.get(\"target_outlier_%\"),\n",
    "                \"Target CV\": metadata.get(\"target_coefficient_of_variation\"),\n",
    "                \"Target Unique Values\": metadata.get(\"target_unique_values\"),\n",
    "                \"Top Correlated Feature\": top_corr_name,\n",
    "                \"Top Correlation Value\": top_corr_value,\n",
    "                \"Domain\": metadata.get(\"domain\", \"unknown\")\n",
    "            })\n",
    "\n",
    "    df_summary = pd.DataFrame(summary)\n",
    "    df_summary.sort_values(by=[\"Num Observations\", \"Num Features\"], inplace=True)\n",
    "\n",
    "    output_path = os.path.join(output_table_directory, \"datasets_summary_table.csv\")\n",
    "    df_summary.to_csv(output_path, index=False)\n",
    "    print(f\"üìä Tabela zbiorcza zapisana w: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_regression(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocessing dataset dla problemu regresji.\n",
    "    Wykonywane kroki:\n",
    "      1. Usuniƒôcie duplikat√≥w.\n",
    "      2. Usuniƒôcie kolumn (poza targetem) zawierajƒÖcych tylko jednƒÖ unikalnƒÖ warto≈õƒá.\n",
    "      3. Imputacja brakujƒÖcych warto≈õci:\n",
    "         - cechy numeryczne: ≈õrednia,\n",
    "         - cechy kategoryczne: warto≈õƒá 'Missing'.\n",
    "      4. One-hot encoding cech kategorycznych.\n",
    "      5. Minmax wszystkich cech (po transformacjach).\n",
    "    \"\"\"\n",
    "    # Krok 1: Usuniƒôcie duplikat√≥w\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Krok 2: Usuniƒôcie kolumn z jednƒÖ unikalnƒÖ warto≈õciƒÖ (pomijamy target)\n",
    "    cols_to_drop = [col for col in df.columns if col != target_col and df[col].nunique() == 1]\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # Oddzielenie cech od zmiennej docelowej\n",
    "    y = df[target_col]\n",
    "    X = df.drop(columns=[target_col])\n",
    "    \n",
    "    # Rozpoznanie kolumn numerycznych i kategorycznych\n",
    "    num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Budowa transformera:\n",
    "    # - Cechy numeryczne: imputacja ≈õredniƒÖ.\n",
    "    # - Cechy kategoryczne: imputacja brak√≥w sta≈ÇƒÖ warto≈õciƒÖ 'Missing' + one-hot encoding.\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', SimpleImputer(strategy='mean'), num_cols),\n",
    "            ('cat', Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "                ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "            ]), cat_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Pipeline: najpierw transformacja, i minmax scaler\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "    \n",
    "    # Dopasowanie i transformacja cech\n",
    "    X_processed = pipeline.fit_transform(X)\n",
    "    \n",
    "    # Odtworzenie nazw kolumn\n",
    "    new_columns = []\n",
    "    new_columns.extend(num_cols)\n",
    "    if cat_cols:\n",
    "        onehot = pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot']\n",
    "        new_cat_cols = onehot.get_feature_names_out(cat_cols)\n",
    "        new_columns.extend(new_cat_cols)\n",
    "    \n",
    "    X_processed = pd.DataFrame(X_processed, columns=new_columns, index=X.index)\n",
    "    \n",
    "    # Po≈ÇƒÖczenie przetworzonych cech z oryginalnƒÖ kolumnƒÖ target\n",
    "    df_processed = X_processed.copy()\n",
    "    df_processed[target_col] = y\n",
    "    \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datasets(input_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Funkcja przechodzi przez wszystkie pliki CSV w katalogu input_path.\n",
    "    Dla ka≈ºdego pliku:\n",
    "      - Odczytuje odpowiadajƒÖcy plik metadanych (.meta.json) i wyciƒÖga nazwƒô kolumny target.\n",
    "      - Wczytuje dane z CSV.\n",
    "      - Przetwarza dane za pomocƒÖ funkcji preprocess_dataset_regression.\n",
    "      - Zapisuje wynikowy DataFrame jako CSV do katalogu output_path przy zachowaniu oryginalnej nazwy pliku.\n",
    "      - Kopiuje r√≥wnie≈º plik metadanych (.meta.json) do katalogu wyj≈õciowego.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    for file in os.listdir(input_path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv_path = os.path.join(input_path, file)\n",
    "            base_name = os.path.splitext(file)[0]\n",
    "            meta_filename = base_name + \".meta.json\"\n",
    "            meta_path = os.path.join(input_path, meta_filename)\n",
    "            \n",
    "            if not os.path.exists(meta_path):\n",
    "                print(f\"Brak pliku metadanych dla {file}. Pomijam plik.\")\n",
    "                continue\n",
    "            \n",
    "            # Wczytanie metadanych\n",
    "            with open(meta_path, \"r\") as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            target_col = metadata.get(\"target\")\n",
    "            if not target_col:\n",
    "                print(f\"Nie znaleziono nazwy kolumny target w metadanych dla {file}. Pomijam plik.\")\n",
    "                continue\n",
    "            \n",
    "            # Wczytanie danych\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Nie uda≈Ço siƒô wczytaƒá pliku {csv_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Przetwarzanie danych\n",
    "            try:\n",
    "                df_processed = preprocess_dataset_regression(df, target_col)\n",
    "            except Exception as e:\n",
    "                print(f\"B≈ÇƒÖd przy przetwarzaniu {csv_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Zapis przetworzonego CSV\n",
    "            output_csv_path = os.path.join(output_path, file)\n",
    "            try:\n",
    "                df_processed.to_csv(output_csv_path, index=False)\n",
    "                print(f\"‚úî Przetworzono i zapisano: {output_csv_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Nie uda≈Ço siƒô zapisaƒá pliku {output_csv_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Kopiowanie pliku metadanych do katalogu wyj≈õciowego\n",
    "            output_meta_path = os.path.join(output_path, meta_filename)\n",
    "            try:\n",
    "                shutil.copy(meta_path, output_meta_path)\n",
    "                print(f\"üìÅ Skopiowano metadane: {output_meta_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Nie uda≈Ço siƒô skopiowaƒá metadanych dla {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_ids = [\n",
    "    # 1,\n",
    "    # 374,\n",
    "    # 320,\n",
    "    # 294,\n",
    "    186,\n",
    "    # 242\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî Saved CSV: ./uci_datasets\\Wine_Quality_186.csv\n",
      "‚úî Saved Metadata: ./uci_datasets\\Wine_Quality_186.meta.json\n",
      "üìä Tabela zbiorcza zapisana w: ./datasets_summary\\datasets_summary_table.csv\n"
     ]
    }
   ],
   "source": [
    "download_uci_datasets(\n",
    "    dataset_ids=datasets_ids,\n",
    "    directory_path=\"./uci_datasets\"\n",
    ")\n",
    "generate_dataset_summary_table(\n",
    "    input_datasets_directory=\"./uci_datasets\",\n",
    "    output_table_directory=\"./datasets_summary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Source</th>\n",
       "      <th>ID/Code</th>\n",
       "      <th>Num Observations</th>\n",
       "      <th>Num Features</th>\n",
       "      <th>Categorical Features</th>\n",
       "      <th>Numeric Features</th>\n",
       "      <th>% Null Values</th>\n",
       "      <th>Target</th>\n",
       "      <th>Target Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Target Max</th>\n",
       "      <th>Target Std Dev</th>\n",
       "      <th>Target IQR</th>\n",
       "      <th>Target Skewness</th>\n",
       "      <th>Target Outlier %</th>\n",
       "      <th>Target CV</th>\n",
       "      <th>Target Unique Values</th>\n",
       "      <th>Top Correlated Feature</th>\n",
       "      <th>Top Correlation Value</th>\n",
       "      <th>Domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Appliances Energy Prediction</td>\n",
       "      <td>UCI</td>\n",
       "      <td>374</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combined Cycle Power Plant</td>\n",
       "      <td>UCI</td>\n",
       "      <td>294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Energy Efficiency</td>\n",
       "      <td>UCI</td>\n",
       "      <td>242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student Performance</td>\n",
       "      <td>UCI</td>\n",
       "      <td>320</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wine Quality</td>\n",
       "      <td>UCI</td>\n",
       "      <td>186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Dataset Name Source  ID/Code  Num Observations  \\\n",
       "0  Appliances Energy Prediction    UCI      374               NaN   \n",
       "1    Combined Cycle Power Plant    UCI      294               NaN   \n",
       "2             Energy Efficiency    UCI      242               NaN   \n",
       "3           Student Performance    UCI      320               NaN   \n",
       "4                  Wine Quality    UCI      186               NaN   \n",
       "\n",
       "   Num Features  Categorical Features  Numeric Features  % Null Values  \\\n",
       "0           NaN                     0                 0            NaN   \n",
       "1           NaN                     0                 0            NaN   \n",
       "2           NaN                     0                 0            NaN   \n",
       "3           NaN                     0                 0            NaN   \n",
       "4           NaN                     0                 0            NaN   \n",
       "\n",
       "   Target  Target Mean  ...  Target Max  Target Std Dev  Target IQR  \\\n",
       "0     NaN          NaN  ...         NaN             NaN         NaN   \n",
       "1     NaN          NaN  ...         NaN             NaN         NaN   \n",
       "2     NaN          NaN  ...         NaN             NaN         NaN   \n",
       "3     NaN          NaN  ...         NaN             NaN         NaN   \n",
       "4     NaN          NaN  ...         NaN             NaN         NaN   \n",
       "\n",
       "   Target Skewness  Target Outlier %  Target CV  Target Unique Values  \\\n",
       "0              NaN               NaN        NaN                   NaN   \n",
       "1              NaN               NaN        NaN                   NaN   \n",
       "2              NaN               NaN        NaN                   NaN   \n",
       "3              NaN               NaN        NaN                   NaN   \n",
       "4              NaN               NaN        NaN                   NaN   \n",
       "\n",
       "   Top Correlated Feature  Top Correlation Value   Domain  \n",
       "0                     NaN                    NaN  unknown  \n",
       "1                     NaN                    NaN  unknown  \n",
       "2                     NaN                    NaN  unknown  \n",
       "3                     NaN                    NaN  unknown  \n",
       "4                     NaN                    NaN  unknown  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df = pd.read_csv(\"datasets_summary/datasets_summary_table.csv\")\n",
    "summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_datasets(\n",
    "#     input_path=\"./uci_datasets\",\n",
    "#     output_path=\"./uci_datasets_preprocessed\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# === INPUTS ===\n",
    "target_col = 'target'  # nazwa kolumny target\n",
    "columns_to_drop = ['col1', 'col2']  # kolumny do usuniƒôcia\n",
    "columns_to_label_fill = ['col3', 'col4']  # kolumny, gdzie zamieniamy NaN na nowy label\n",
    "label_fill_value = 'missing'  # nowa warto≈õƒá do podstawienia\n",
    "columns_to_mean_fill = ['col5', 'col6']  # kolumny, gdzie uzupe≈Çniamy NaN ≈õredniƒÖ\n",
    "columns_to_onehot = ['col7', 'col8']\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame,log_path) -> pd.DataFrame:\n",
    "    # 1. Drop specified columns\n",
    "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    # 2. Drop duplicate rows\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # 3. Drop columns with constant value\n",
    "    nunique = df.nunique()\n",
    "    constant_cols = nunique[nunique <= 1].index.tolist()\n",
    "    df = df.drop(columns=constant_cols)\n",
    "\n",
    "    # 4. Replace NaNs in specified columns with label\n",
    "    for col in columns_to_label_fill:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(label_fill_value)\n",
    "\n",
    "    # 5. Replace NaNs in other specified columns with mean\n",
    "    for col in columns_to_mean_fill:\n",
    "        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "    # 6. One-hot encode specified columns\n",
    "    df = pd.get_dummies(df, columns=[col for col in columns_to_onehot if col in df.columns], drop_first=False,dtype=int)\n",
    "\n",
    "    # 7. Standardize all features\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.difference([target_col])\n",
    "    scaler = StandardScaler()\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    with open(log_path, \"w\") as f:\n",
    "        f.write('0. Target column: {}\\n\\n'.format(target_col))\n",
    "\n",
    "        f.write(\"1. Dropped columns:\\n\")\n",
    "        f.write(\", \".join(columns_to_drop) + \"\\n\\n\")\n",
    "\n",
    "        f.write(\"2. Columns with constant values (removed after checking):\\n\")\n",
    "        f.write(\", \".join(constant_cols) + \"\\n\\n\")\n",
    "\n",
    "        f.write(\"3. Columns with missing values filled with label '{}':\\n\".format(label_fill_value))\n",
    "        f.write(\", \".join(columns_to_label_fill) + \"\\n\\n\")\n",
    "\n",
    "        f.write(\"4. Columns with missing values filled with mean:\\n\")\n",
    "        f.write(\", \".join(columns_to_mean_fill) + \"\\n\\n\")\n",
    "\n",
    "        f.write(\"5. Columns one-hot encoded:\\n\")\n",
    "        f.write(\", \".join(columns_to_onehot) + \"\\n\\n\")\n",
    "\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality color  \n",
       "0      9.4        5   red  \n",
       "1      9.8        5   red  \n",
       "2      9.8        5   red  \n",
       "3      9.8        6   red  \n",
       "4      9.4        5   red  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"Wine_Quality_186\"\n",
    "df = pd.read_csv(f\"./uci_datasets/{dataset_name}.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_cols :\n",
      "['color']\n",
      "\n",
      "numerical_cols :\n",
      "['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Print w formacie do ≈Çatwego kopiowania\n",
    "print(\"categorical_cols :\")\n",
    "print(categorical_cols)\n",
    "print()\n",
    "print(\"numerical_cols :\")\n",
    "print(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'quality'\n",
    "\n",
    "columns_to_drop = []\n",
    "\n",
    "columns_to_label_fill = ['color']\n",
    "\n",
    "label_fill_value = 'missing'\n",
    "\n",
    "columns_to_mean_fill = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
    "\n",
    "columns_to_onehot = ['color']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>color_red</th>\n",
       "      <th>color_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.140064</td>\n",
       "      <td>2.115349</td>\n",
       "      <td>-2.164515</td>\n",
       "      <td>-0.699699</td>\n",
       "      <td>0.523880</td>\n",
       "      <td>-1.069272</td>\n",
       "      <td>-1.411143</td>\n",
       "      <td>1.100996</td>\n",
       "      <td>1.779304</td>\n",
       "      <td>0.177941</td>\n",
       "      <td>-0.969152</td>\n",
       "      <td>5</td>\n",
       "      <td>1.707233</td>\n",
       "      <td>-1.707233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.443199</td>\n",
       "      <td>3.185297</td>\n",
       "      <td>-2.164515</td>\n",
       "      <td>-0.544135</td>\n",
       "      <td>1.120736</td>\n",
       "      <td>-0.282905</td>\n",
       "      <td>-0.829839</td>\n",
       "      <td>0.763753</td>\n",
       "      <td>-0.153797</td>\n",
       "      <td>0.979389</td>\n",
       "      <td>-0.631833</td>\n",
       "      <td>5</td>\n",
       "      <td>1.707233</td>\n",
       "      <td>-1.707233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.443199</td>\n",
       "      <td>2.471998</td>\n",
       "      <td>-1.892672</td>\n",
       "      <td>-0.610806</td>\n",
       "      <td>0.957957</td>\n",
       "      <td>-0.844596</td>\n",
       "      <td>-1.058837</td>\n",
       "      <td>0.831202</td>\n",
       "      <td>0.220351</td>\n",
       "      <td>0.779027</td>\n",
       "      <td>-0.631833</td>\n",
       "      <td>5</td>\n",
       "      <td>1.707233</td>\n",
       "      <td>-1.707233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.019841</td>\n",
       "      <td>-0.381197</td>\n",
       "      <td>1.641293</td>\n",
       "      <td>-0.699699</td>\n",
       "      <td>0.496751</td>\n",
       "      <td>-0.732258</td>\n",
       "      <td>-0.953146</td>\n",
       "      <td>1.168444</td>\n",
       "      <td>-0.403229</td>\n",
       "      <td>0.311515</td>\n",
       "      <td>-0.631833</td>\n",
       "      <td>6</td>\n",
       "      <td>1.707233</td>\n",
       "      <td>-1.707233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.140064</td>\n",
       "      <td>1.877583</td>\n",
       "      <td>-2.164515</td>\n",
       "      <td>-0.721923</td>\n",
       "      <td>0.496751</td>\n",
       "      <td>-0.956934</td>\n",
       "      <td>-1.305451</td>\n",
       "      <td>1.100996</td>\n",
       "      <td>1.779304</td>\n",
       "      <td>0.177941</td>\n",
       "      <td>-0.969152</td>\n",
       "      <td>5</td>\n",
       "      <td>1.707233</td>\n",
       "      <td>-1.707233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0       0.140064          2.115349    -2.164515       -0.699699   0.523880   \n",
       "1       0.443199          3.185297    -2.164515       -0.544135   1.120736   \n",
       "2       0.443199          2.471998    -1.892672       -0.610806   0.957957   \n",
       "3       3.019841         -0.381197     1.641293       -0.699699   0.496751   \n",
       "5       0.140064          1.877583    -2.164515       -0.721923   0.496751   \n",
       "\n",
       "   free_sulfur_dioxide  total_sulfur_dioxide   density        pH  sulphates  \\\n",
       "0            -1.069272             -1.411143  1.100996  1.779304   0.177941   \n",
       "1            -0.282905             -0.829839  0.763753 -0.153797   0.979389   \n",
       "2            -0.844596             -1.058837  0.831202  0.220351   0.779027   \n",
       "3            -0.732258             -0.953146  1.168444 -0.403229   0.311515   \n",
       "5            -0.956934             -1.305451  1.100996  1.779304   0.177941   \n",
       "\n",
       "    alcohol  quality  color_red  color_white  \n",
       "0 -0.969152        5   1.707233    -1.707233  \n",
       "1 -0.631833        5   1.707233    -1.707233  \n",
       "2 -0.631833        5   1.707233    -1.707233  \n",
       "3 -0.631833        6   1.707233    -1.707233  \n",
       "5 -0.969152        5   1.707233    -1.707233  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df = preprocess_dataframe(df, log_path=f\"./uci_datasets_logs/{dataset_name}_preprocessing_log.txt\")\n",
    "preprocessed_df.to_csv(f\"./uci_datasets_preprocessed/{dataset_name}_preprocessed.csv\", index=False)\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
